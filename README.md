# CS9692 Machine Learning and Optimization

This repository was created for CS9692 Machine Learning and Optimization Project. 

The focus of our project was adding an adaptive gradient descent method (ADAM) to F2SA algorithm to increase the convergence speed while solving bilevel optimization problems.

Our implementation of the F2SA, SABA, and StocBiO algorithm is based on the code authored by (Dagr ́eou et al., 2022), as made available in their code repository https://github.com/benchopt/benchmark_bilevel. We added a new FashionMNIST dataset and the proposed optimizer – F2SA with Adam to it. We also corrected the definition of the accuracy in their repository, as the original one accounted for error percentage instead.


